{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l8LOb8IbYxg"
   },
   "source": [
    "## GNN Model Code\n",
    "\n",
    "GNN model using molecular scent dataset from Leffingwell Odor Datset (loaded using Pyrfume - https://pyrfume.org)\n",
    "\n",
    "Code below modified from example code given in the \"Predicting DFT Energies with GNNs\", \"Interpretability and Deep Learning\" sections of \"Deep Learning for Molecules and Materials\" textbook (https://dmol.pub/applied/QM9.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZeyyFPJbYxi",
    "outputId": "2ac9b9a9-ca8b-4608-d3a4-a2c8f937f71f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyrfume\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Plotting style\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Related Code\n",
    "NOTE: numEpochs is currently set to 2 & the dataset is downsampled, edit code when actually training/evaluating model to use the entire dataset & a larger number of empochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model inputs and hyperparameters\n",
    "learning_rate = 1e-5\n",
    "num_Dense_layers = 2\n",
    "num_GNN_layers = 4\n",
    "# NOTE: currently using reduced number of epochs, increase when training model (in paper used 138 epochs)\n",
    "numEpochs = (\n",
    "    2  # reduced value for checking notebook, edit when training/evaluating the model\n",
    ")\n",
    "steps_for_gradUpdate = 8\n",
    "graph_feat_length = 512\n",
    "node_feat_length = 256\n",
    "message_feat_length = node_feat_length\n",
    "weights_stddevGNN = 1e-2\n",
    "earlyStopping = True\n",
    "earlyStopping_patience = 3\n",
    "earlyStopping_minDelta = 0\n",
    "regularizationStrength = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train-test split given in Leffingwell Dataset - except add in validation set (have 70% train, 10% validation, 20% test rather than 80% train & 20% test)\n",
    "# Load data\n",
    "scentdata = pyrfume.load_data(\"leffingwell/leffingwell_data.csv\", remote=True)\n",
    "\n",
    "# Code used to create train, test & validation sets (based on the splits given in Leffingwell Dataset)\n",
    "testData = scentdata[scentdata[\"labels_train/test\"] == 0]\n",
    "numTestData = len(testData)\n",
    "\n",
    "trainAndValidationData = scentdata[scentdata[\"labels_train/test\"] == 1]\n",
    "\n",
    "numTrainAndValidationData = len(trainAndValidationData)\n",
    "trainAndValidationData = trainAndValidationData.reset_index()\n",
    "\n",
    "numMoleculesInDataset = numTestData + numTrainAndValidationData\n",
    "\n",
    "# randomly select indices from trainAndValidation data - validation set = 10% of entire dataset\n",
    "numValidationData = int(0.10 * numMoleculesInDataset)\n",
    "validationIndices = np.random.choice(\n",
    "    a=numTrainAndValidationData, size=numValidationData, replace=False\n",
    ")  # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "validationData = trainAndValidationData.iloc[\n",
    "    validationIndices\n",
    "]  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
    "trainData = trainAndValidationData.drop(\n",
    "    index=validationIndices\n",
    ")  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "\n",
    "\n",
    "# Use smaller set of data (just to check code does not crash)\n",
    "# NOTE: comment out the next 3 lines when actually training/evaluating model to use the entire dataset\n",
    "trainData = trainData.sample(frac=0.01, random_state=0).reset_index(drop=True)\n",
    "validationData = validationData.sample(frac=0.01, random_state=0).reset_index(drop=True)\n",
    "testData = testData.sample(frac=0.8, random_state=0).reset_index(\n",
    "    drop=True\n",
    ")  # Sample more from test set to avoid error that there is only 1 molecule with a certain scent when calculating AUROC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate list of all scent labels (scentClasses)\n",
    "numMolecules = len(scentdata.odor_labels_filtered)\n",
    "numClasses = 112  # No odorless class\n",
    "scentClasses = pd.read_csv(\"scentClasses.csv\")\n",
    "scentClasses = scentClasses[\"Scent\"].tolist()\n",
    "moleculeScentList = []\n",
    "for i in range(numMolecules):\n",
    "    scentString = scentdata.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList.append(scentList)\n",
    "\n",
    "# Generate moleculeScentList_train, moleculeScentList_test, moleculeScentList_validation\n",
    "numTrainMolecules = len(trainData.odor_labels_filtered)\n",
    "moleculeScentList_train = []\n",
    "for i in range(numTrainMolecules):\n",
    "    scentString = trainData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_train.append(scentList)\n",
    "\n",
    "numValidationMolecules = len(validationData.odor_labels_filtered)\n",
    "moleculeScentList_validation = []\n",
    "for i in range(numValidationMolecules):\n",
    "    scentString = validationData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_validation.append(scentList)\n",
    "\n",
    "numTestMolecules = len(testData.odor_labels_filtered)\n",
    "moleculeScentList_test = []\n",
    "for i in range(numTestMolecules):\n",
    "    scentString = testData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_test.append(scentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3DUaY-hbYxm",
    "outputId": "2d3fa50a-7ec5-4e6a-e15f-7f43b711dba5"
   },
   "outputs": [],
   "source": [
    "def gen_smiles2graph(sml):\n",
    "    \"\"\"Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    \"\"\"\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {\n",
    "        rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "        rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "        rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "        rdkit.Chem.rdchem.BondType.AROMATIC: 4,\n",
    "    }\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N, node_feat_length))\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), i.GetAtomicNum()] = 1\n",
    "        # Add in whether atom is in a ring or not for one-hot encoding\n",
    "        if i.IsInRing():\n",
    "            nodes[i.GetIdx(), -1] = 1\n",
    "\n",
    "    adj = np.zeros((N, N))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning(\"Ignoring bond order\" + order)\n",
    "        adj[u, v] = 1\n",
    "        adj[v, u] = 1\n",
    "    adj += np.eye(N)\n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f32kIuxbYxo"
   },
   "outputs": [],
   "source": [
    "# Function that creates label vector given list of strings describing scent of molecule as input\n",
    "# Each index in label vector corresponds to specific scent -> if output has a 0 at index i, then molecule does not have scent i\n",
    "# If label vector has 1 at index i, then molecule does have scent i\n",
    "\n",
    "\n",
    "def createLabelVector(scentsList):\n",
    "    # Find class index in label vector that each scent corresponds to & update label for that molecule to 1\n",
    "    labelVector = np.zeros(numClasses)\n",
    "    for j in range(len(scentsList)):\n",
    "        # Find class index\n",
    "        classIndex = scentClasses.index(scentsList[j])\n",
    "        # print(classIndex)\n",
    "        # print(scentsList[j])\n",
    "        # print(scentClasses[classIndex])\n",
    "        # Update label vector\n",
    "        labelVector[classIndex] = 1\n",
    "    return labelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68fbM76wbYxp"
   },
   "outputs": [],
   "source": [
    "def generateGraphsTrain():\n",
    "    for i in range(numTrainMolecules):\n",
    "        graph = gen_smiles2graph(trainData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_train[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphsValidation():\n",
    "    for i in range(numValidationMolecules):\n",
    "        graph = gen_smiles2graph(validationData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_validation[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphsTest():\n",
    "    for i in range(numTestMolecules):\n",
    "        graph = gen_smiles2graph(testData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_test[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphs():\n",
    "    for i in range(numMolecules):\n",
    "        graph = gen_smiles2graph(scentdata.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "# Check that generateGraphs() works for 1st molecule\n",
    "# print(gen_smiles2graph(scentdata.SMILES[0]))\n",
    "# print(scentdata.SENTENCE[0].split(','))\n",
    "# print(np.nonzero(createLabelVector(scentdata.SENTENCE[0].split(','))))\n",
    "# print(scentClasses[89])\n",
    "\n",
    "data = tf.data.Dataset.from_generator(\n",
    "    generateGraphs,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "train_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsTrain,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "valid_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsValidation,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsTest,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyI7FfjwbYxq"
   },
   "outputs": [],
   "source": [
    "train_N = numTrainMolecules\n",
    "valid_N = numValidationMolecules\n",
    "test_N = numTestMolecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHYC-317bYxq"
   },
   "outputs": [],
   "source": [
    "class GNNLayer(hk.Module):\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # split input into nodes, edges & features\n",
    "        nodes, edges, features = inputs\n",
    "        # Nodes is of shape (N, Nf) --> N = # atoms, Nf = node_feature_length\n",
    "        # Edges is of shape (N,N) (adjacency matrix)\n",
    "        # Features is of shape (Gf) --> Gf = graph_feature_length\n",
    "\n",
    "        graph_feature_len = features.shape[-1]  # graph_feature_len (Gf)\n",
    "        node_feature_len = nodes.shape[-1]  # node_feature_len (Nf)\n",
    "        message_feature_len = message_feat_length  # message_feature_length (Mf)\n",
    "\n",
    "        # Initialize weights\n",
    "        w_init = hk.initializers.RandomNormal(stddev=weights_stddevGNN)\n",
    "\n",
    "        # we is of shape (Nf,Mf)\n",
    "        we = hk.get_parameter(\n",
    "            \"we\", shape=[node_feature_len, message_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # b is of shape (Mf)\n",
    "        b = hk.get_parameter(\"b\", shape=[message_feature_len], init=w_init)\n",
    "\n",
    "        # wv is of shape (Mf,Nf)\n",
    "        wv = hk.get_parameter(\n",
    "            \"wv\", shape=[message_feature_len, node_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # wu is of shape (Nf,Gf)\n",
    "        wu = hk.get_parameter(\n",
    "            \"wu\", shape=[node_feature_len, graph_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # make nodes be N x N x Nf so we can just multiply directly (N = number of atoms)\n",
    "        # ek is now shaped N x N x Mf\n",
    "        ek = jax.nn.leaky_relu(\n",
    "            b\n",
    "            + jnp.repeat(nodes[jnp.newaxis, ...], nodes.shape[0], axis=0)\n",
    "            @ we\n",
    "            * edges[..., None]\n",
    "        )\n",
    "\n",
    "        # Uncomment lines below to update edges (also edit return line so new_edges is returned)\n",
    "        # Update edges, use jnp.any to have new_edges be of shape N x N\n",
    "        # new_edges = jnp.any(ek, axis=-1)\n",
    "\n",
    "        # Normalize over edge features w/layer normalization\n",
    "        # new_edges = hk.LayerNorm(axis=[0,1], create_scale=False, create_offset=False, eps=1e-05)(new_edges)\n",
    "\n",
    "        # take sum over neighbors to get ebar shape = Nf x Mf\n",
    "        ebar = jnp.sum(ek, axis=1)\n",
    "\n",
    "        # dense layer for new nodes to get new_nodes shape = N x Nf\n",
    "        new_nodes = jax.nn.leaky_relu(ebar @ wv) + nodes  # Use leaky ReLU\n",
    "\n",
    "        # Normalize over node features w/layer normalization\n",
    "        new_nodes = hk.LayerNorm(\n",
    "            axis=[0, 1], create_scale=False, create_offset=False, eps=1e-05\n",
    "        )(new_nodes)\n",
    "\n",
    "        # sum over nodes to get shape features so global_node_features shape = Nf\n",
    "        global_node_features = jnp.sum(new_nodes, axis=0)\n",
    "\n",
    "        # dense layer for new features so new_features shape = Gf\n",
    "        new_features = (\n",
    "            jax.nn.leaky_relu(global_node_features @ wu) + features\n",
    "        )  # Use leaky ReLU for activation\n",
    "\n",
    "        return new_nodes, edges, new_features\n",
    "\n",
    "\n",
    "def model_fn(x):\n",
    "    nodes, edges = x\n",
    "    features = jnp.ones(graph_feat_length)\n",
    "    x = nodes, edges, features\n",
    "\n",
    "    # NOTE: If edited num_GNN_layers, need to edit code below (increase or decrease # times have x = GNNLayer(...))\n",
    "    # 4 GNN layers\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "\n",
    "    # 2 dense layers\n",
    "    logits = hk.Linear(numClasses)(x[-1])\n",
    "    logits = hk.Linear(numClasses)(logits)\n",
    "\n",
    "    return logits  # Model now returns logits\n",
    "\n",
    "\n",
    "model = hk.without_apply_rng(hk.transform(model_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHYC-317bYxq"
   },
   "outputs": [],
   "source": [
    "# from https://dmol.pub/dl/xai.html\n",
    "def cross_entropy_logits(logits, y):\n",
    "    return jnp.mean(\n",
    "        jnp.clip(logits, 0, None) - logits * y + jnp.log(1 + jnp.exp(-jnp.abs(logits)))\n",
    "    )\n",
    "\n",
    "\n",
    "# Use loss function below if model outputs logits & do not want to use L2 regularization\n",
    "def loss_fn_logits(params, x, y):\n",
    "    logits = model.apply(params, x)\n",
    "    return cross_entropy_logits(logits, y)\n",
    "\n",
    "\n",
    "# Use loss function below if model outputs logits & want to include L2 regularization\n",
    "# Code to compute L2 regularization based on that in the \"MLP on MNIST\" Example on the Haiku Github repository (https://github.com/deepmind/dm-haiku/blob/main/examples/mnist.py)\n",
    "def loss_fn_logits_reg(params, x, y):\n",
    "    l2_lossTerm = regularizationStrength * sum(\n",
    "        jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params)\n",
    "    )\n",
    "    logits = loss_fn_logits(params, x, y)\n",
    "    return logits + l2_lossTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WkcIX4_bYxs",
    "outputId": "eb398112-a6a5-49e9-dbdd-172f63977410"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "sampleData = data.take(1)\n",
    "\n",
    "for dataVal in sampleData:\n",
    "    (nodes_i, edges_i), yi = dataVal\n",
    "nodes_i = nodes_i.numpy()\n",
    "edges_i = edges_i.numpy()\n",
    "\n",
    "yi = yi.numpy()\n",
    "xi = (nodes_i, edges_i)\n",
    "\n",
    "params = model.init(rng, xi)\n",
    "\n",
    "opt_init, opt_update = optax.chain(\n",
    "    optax.apply_every(k=steps_for_gradUpdate), optax.adam(learning_rate)\n",
    ")\n",
    "\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(opt_state, x, y, params):\n",
    "    value, grads = jax.value_and_grad(loss_fn_logits_reg)(params, x, y)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    updated_params = optax.apply_updates(params, updates)\n",
    "    return value, opt_state, updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = numEpochs\n",
    "print(\n",
    "    f\"Number of Epochs: {epochs}, learning rate: {learning_rate}, node_feature_len: {node_feat_length}, graph_feature_len: {graph_feat_length}, message_feature_length: {message_feat_length}, {num_Dense_layers} Dense, {num_GNN_layers} GNN layers\"\n",
    ")\n",
    "val_loss = np.zeros(epochs)\n",
    "train_loss = np.zeros(epochs)\n",
    "\n",
    "# early stopping counter\n",
    "counter = 0\n",
    "epochStoppedAt = epochs\n",
    "\n",
    "for e in range(epochs):\n",
    "    if counter == earlyStopping_patience:\n",
    "        print(f\"Early stopping, stopped at Epoch {e} (Note 1st epoch = 0)\")\n",
    "        epochStoppedAt = e\n",
    "        break  # Early stopping\n",
    "\n",
    "    for i, elementInTrainSet in enumerate(train_set):\n",
    "        (ni, ei), yi = elementInTrainSet\n",
    "        ni = ni.numpy()\n",
    "        ei = ei.numpy()\n",
    "        yi = yi.numpy()\n",
    "        xi = ni, ei\n",
    "        value, opt_state, params = update(opt_state, xi, yi, params)\n",
    "        train_loss[e] += value\n",
    "\n",
    "    train_loss[e] = train_loss[e] / train_N  # Take average loss over all molecules\n",
    "    print(f\"Training Loss, Epoch {e}: {train_loss[e]}\")\n",
    "\n",
    "    for j, v in enumerate(valid_set):\n",
    "        (n_val, e_val), y = v\n",
    "        n_val = n_val.numpy()\n",
    "        e_val = e_val.numpy()\n",
    "        y = y.numpy()\n",
    "        x = n_val, e_val\n",
    "        loss = loss_fn_logits_reg(params, x, y)\n",
    "        val_loss[e] += loss\n",
    "\n",
    "    val_loss[e] = val_loss[e] / valid_N  # Take average loss over all molecules\n",
    "\n",
    "    # Check if have improvement/increase in validation loss (early stopping)\n",
    "    if e > 0:\n",
    "        lossDiff = (\n",
    "            prevValidLoss - val_loss[e]\n",
    "        )  # If have improvement, prevValidLoss > val_loss[e]\n",
    "        if lossDiff < earlyStopping_minDelta:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "\n",
    "    prevValidLoss = val_loss[e]\n",
    "\n",
    "    print(f\"Epoch {e}, Validation Loss: {val_loss[e]}\")\n",
    "\n",
    "\n",
    "opt_params = params\n",
    "# Save optimal parameters\n",
    "opt_params_flattened = jax.tree_util.tree_flatten(opt_params)\n",
    "# fileName = f'optParams_{epochs}Epochs.npy'\n",
    "# np.save(fileName, opt_params_flattened[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 2 cells below if reading model parameters from file & have not run earlier cells in the notebook\n",
    "NOTE: the dataset is downsampled, edit code when actually training/evaluating model to use the entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If reading model parameters from file (rather than computing metrics directly after training), run 2 cells below\n",
    "\n",
    "# Parameters for GNN model (parameters being read in)\n",
    "node_feat_length = 256\n",
    "message_feat_length = 256\n",
    "graph_feat_length = 512\n",
    "weights_stddevGNN = 0.01\n",
    "\n",
    "# Use train-test split given in Leffingwell Dataset - except add in validation set (have 70% train, 10% validation, 20% test rather than 80% train & 20% test)\n",
    "# Load data\n",
    "scentdata = pyrfume.load_data(\"leffingwell/leffingwell_data.csv\", remote=True)\n",
    "\n",
    "# Code used to create train, test & validation sets (based on the splits given in Leffingwell Dataset) & write to csv file\n",
    "testData = scentdata[scentdata[\"labels_train/test\"] == 0]\n",
    "numTestData = len(testData)\n",
    "\n",
    "trainAndValidationData = scentdata[scentdata[\"labels_train/test\"] == 1]\n",
    "\n",
    "numTrainAndValidationData = len(trainAndValidationData)\n",
    "trainAndValidationData = trainAndValidationData.reset_index()\n",
    "\n",
    "numMoleculesInDataset = numTestData + numTrainAndValidationData\n",
    "\n",
    "# randomly select indices from trainAndValidation data - validation set = 10% of entire dataset\n",
    "numValidationData = int(0.10 * numMoleculesInDataset)\n",
    "validationIndices = np.random.choice(\n",
    "    a=numTrainAndValidationData, size=numValidationData, replace=False\n",
    ")  # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "validationData = trainAndValidationData.iloc[\n",
    "    validationIndices\n",
    "]  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
    "trainData = trainAndValidationData.drop(\n",
    "    index=validationIndices\n",
    ")  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "\n",
    "\n",
    "# Use smaller set of data (just to check code does not crash)\n",
    "# NOTE: comment out the next 3 lines when actually training/evaluating model to use the entire dataset\n",
    "trainData = trainData.sample(frac=0.01, random_state=0).reset_index(drop=True)\n",
    "validationData = validationData.sample(frac=0.01, random_state=0).reset_index(drop=True)\n",
    "# testData = testData.sample(frac=0.8, random_state=0).reset_index(\n",
    "#    drop=True\n",
    "# )  # Sample more from test set to avoid error that there is only 1 molecule with a certain scent when calculating AUROC score\n",
    "\n",
    "# Code to generate list of all scent labels (scentClasses)\n",
    "numMolecules = len(scentdata.odor_labels_filtered)\n",
    "numClasses = 112  # No odorless class\n",
    "scentClasses = pd.read_csv(\"scentClasses.csv\")\n",
    "scentClasses = scentClasses[\"Scent\"].tolist()\n",
    "moleculeScentList = []\n",
    "for i in range(numMolecules):\n",
    "    scentString = scentdata.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList.append(scentList)\n",
    "\n",
    "# Generate moleculeScentList_train, moleculeScentList_test, moleculeScentList_validation\n",
    "numTrainMolecules = len(trainData.odor_labels_filtered)\n",
    "moleculeScentList_train = []\n",
    "for i in range(numTrainMolecules):\n",
    "    scentString = trainData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_train.append(scentList)\n",
    "\n",
    "numValidationMolecules = len(validationData.odor_labels_filtered)\n",
    "moleculeScentList_validation = []\n",
    "for i in range(numValidationMolecules):\n",
    "    scentString = validationData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_validation.append(scentList)\n",
    "\n",
    "numTestMolecules = len(testData.odor_labels_filtered)\n",
    "moleculeScentList_test = []\n",
    "for i in range(numTestMolecules):\n",
    "    scentString = testData.odor_labels_filtered[i]\n",
    "    temp = scentString.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.replace(\" \", \"\")\n",
    "    scentList = temp.split(\",\")\n",
    "    if \"odorless\" in scentList:\n",
    "        scentList.remove(\"odorless\")\n",
    "    moleculeScentList_test.append(scentList)\n",
    "\n",
    "\n",
    "def gen_smiles2graph(sml):\n",
    "    \"\"\"Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    \"\"\"\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {\n",
    "        rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "        rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "        rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "        rdkit.Chem.rdchem.BondType.AROMATIC: 4,\n",
    "    }\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N, node_feat_length))\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), i.GetAtomicNum()] = 1\n",
    "        # Add in whether atom is in a ring or not for one-hot encoding\n",
    "        if i.IsInRing():\n",
    "            nodes[i.GetIdx(), -1] = 1\n",
    "\n",
    "    adj = np.zeros((N, N))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning(\"Ignoring bond order\" + order)\n",
    "        adj[u, v] = 1\n",
    "        adj[v, u] = 1\n",
    "    adj += np.eye(N)\n",
    "    return nodes, adj\n",
    "\n",
    "\n",
    "# Function that creates label vector given list of strings describing scent of molecule as input\n",
    "# Each index in label vector corresponds to specific scent -> if output has a 0 at index i, then molecule does not have scent i\n",
    "# If label vector has 1 at index i, then molecule does have scent i\n",
    "\n",
    "\n",
    "def createLabelVector(scentsList):\n",
    "    # Find class index in label vector that each scent corresponds to & update label for that molecule to 1\n",
    "    labelVector = np.zeros(numClasses)\n",
    "    for j in range(len(scentsList)):\n",
    "        # Find class index\n",
    "        classIndex = scentClasses.index(scentsList[j])\n",
    "        # print(classIndex)\n",
    "        # print(scentsList[j])\n",
    "        # print(scentClasses[classIndex])\n",
    "        # Update label vector\n",
    "        labelVector[classIndex] = 1\n",
    "    return labelVector\n",
    "\n",
    "\n",
    "def generateGraphsTrain():\n",
    "    for i in range(numTrainMolecules):\n",
    "        graph = gen_smiles2graph(trainData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_train[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphsValidation():\n",
    "    for i in range(numValidationMolecules):\n",
    "        graph = gen_smiles2graph(validationData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_validation[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphsTest():\n",
    "    for i in range(numTestMolecules):\n",
    "        graph = gen_smiles2graph(testData.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList_test[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "def generateGraphs():\n",
    "    for i in range(numMolecules):\n",
    "        graph = gen_smiles2graph(scentdata.smiles[i])\n",
    "        labels = createLabelVector(moleculeScentList[i])\n",
    "        yield graph, labels\n",
    "\n",
    "\n",
    "# Get graph data for training, testing & validation sets\n",
    "data = tf.data.Dataset.from_generator(\n",
    "    generateGraphs,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "train_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsTrain,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "valid_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsValidation,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_set = tf.data.Dataset.from_generator(\n",
    "    generateGraphsTest,\n",
    "    output_types=((tf.float32, tf.float32), tf.float32),\n",
    "    output_shapes=(\n",
    "        (tf.TensorShape([None, node_feat_length]), tf.TensorShape([None, None])),\n",
    "        tf.TensorShape([None]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_N = numTrainMolecules\n",
    "valid_N = numValidationMolecules\n",
    "test_N = numTestMolecules\n",
    "\n",
    "\n",
    "class GNNLayer(hk.Module):\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # split input into nodes, edges & features\n",
    "        nodes, edges, features = inputs\n",
    "        # Nodes is of shape (N, Nf) --> N = # atoms, Nf = node_feature_length\n",
    "        # Edges is of shape (N,N) (adjacency matrix)\n",
    "        # Features is of shape (Gf) --> Gf = graph_feature_length\n",
    "\n",
    "        graph_feature_len = features.shape[-1]  # graph_feature_len (Gf)\n",
    "        node_feature_len = nodes.shape[-1]  # node_feature_len (Nf)\n",
    "        message_feature_len = message_feat_length  # message_feature_length (Mf)\n",
    "\n",
    "        # Initialize weights\n",
    "        w_init = hk.initializers.RandomNormal(stddev=weights_stddevGNN)\n",
    "\n",
    "        # we is of shape (Nf,Mf)\n",
    "        we = hk.get_parameter(\n",
    "            \"we\", shape=[node_feature_len, message_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # b is of shape (Mf)\n",
    "        b = hk.get_parameter(\"b\", shape=[message_feature_len], init=w_init)\n",
    "\n",
    "        # wv is of shape (Mf,Nf)\n",
    "        wv = hk.get_parameter(\n",
    "            \"wv\", shape=[message_feature_len, node_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # wu is of shape (Nf,Gf)\n",
    "        wu = hk.get_parameter(\n",
    "            \"wu\", shape=[node_feature_len, graph_feature_len], init=w_init\n",
    "        )\n",
    "\n",
    "        # make nodes be N x N x Nf so we can just multiply directly (N = number of atoms)\n",
    "        # ek is now shaped N x N x Mf\n",
    "        ek = jax.nn.leaky_relu(\n",
    "            b\n",
    "            + jnp.repeat(nodes[jnp.newaxis, ...], nodes.shape[0], axis=0)\n",
    "            @ we\n",
    "            * edges[..., None]\n",
    "        )\n",
    "\n",
    "        # Uncomment lines below to update edges (also edit return line so new_edges is returned)\n",
    "        # Update edges, use jnp.any to have new_edges be of shape N x N\n",
    "        # new_edges = jnp.any(ek, axis=-1)\n",
    "\n",
    "        # Normalize over edge features w/layer normalization\n",
    "        # new_edges = hk.LayerNorm(axis=[0,1], create_scale=False, create_offset=False, eps=1e-05)(new_edges)\n",
    "\n",
    "        # take sum over neighbors to get ebar shape = Nf x Mf\n",
    "        ebar = jnp.sum(ek, axis=1)\n",
    "\n",
    "        # dense layer for new nodes to get new_nodes shape = N x Nf\n",
    "        new_nodes = jax.nn.leaky_relu(ebar @ wv) + nodes  # Use leaky ReLU\n",
    "\n",
    "        # Normalize over node features w/layer normalization\n",
    "        new_nodes = hk.LayerNorm(\n",
    "            axis=[0, 1], create_scale=False, create_offset=False, eps=1e-05\n",
    "        )(new_nodes)\n",
    "\n",
    "        # sum over nodes to get shape features so global_node_features shape = Nf\n",
    "        global_node_features = jnp.sum(new_nodes, axis=0)\n",
    "\n",
    "        # dense layer for new features so new_features shape = Gf\n",
    "        new_features = (\n",
    "            jax.nn.leaky_relu(global_node_features @ wu) + features\n",
    "        )  # Use leaky ReLU for activation\n",
    "\n",
    "        # just return features for ease of use\n",
    "        return new_nodes, edges, new_features\n",
    "\n",
    "\n",
    "def model_fn(x):\n",
    "    nodes, edges = x\n",
    "    features = jnp.ones(graph_feat_length)\n",
    "    x = nodes, edges, features\n",
    "\n",
    "    # NOTE: If edited num_GNN_layers, need to edit code below (increase or decrease # times have x = GNNLayer(...))\n",
    "    # 4 GNN layers\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=graph_feat_length)(x)\n",
    "\n",
    "    # 2 dense layers\n",
    "    logits = hk.Linear(numClasses)(x[-1])\n",
    "    # logits = jax.nn.relu(logits) #ReLU activation between dense layer\n",
    "    logits = hk.Linear(numClasses)(logits)\n",
    "\n",
    "    return logits  # Model now returns logits\n",
    "\n",
    "\n",
    "model = hk.without_apply_rng(hk.transform(model_fn))\n",
    "\n",
    "# Initialize model\n",
    "rng = jax.random.PRNGKey(0)\n",
    "sampleData = data.take(1)\n",
    "for dataVal in sampleData:  # Look into later how to get larger set\n",
    "    (nodes_i, edges_i), yi = dataVal\n",
    "nodes_i = nodes_i.numpy()\n",
    "edges_i = edges_i.numpy()\n",
    "\n",
    "yi = yi.numpy()\n",
    "xi = (nodes_i, edges_i)\n",
    "\n",
    "params = model.init(rng, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimal parameters for GNN model\n",
    "print(\"Edit fileName to change parameters being loaded\")\n",
    "fileName = \"optParams_dry-waterfall-17.npy\"  # Currently optimal parameters, edit when get better model\n",
    "paramsArr = jnp.load(fileName, allow_pickle=True)\n",
    "opt_params = {\n",
    "    \"gnn_layer\": {\n",
    "        \"b\": paramsArr[0],\n",
    "        \"we\": paramsArr[1],\n",
    "        \"wu\": paramsArr[2],\n",
    "        \"wv\": paramsArr[3],\n",
    "    },\n",
    "    \"gnn_layer_1\": {\n",
    "        \"b\": paramsArr[4],\n",
    "        \"we\": paramsArr[5],\n",
    "        \"wu\": paramsArr[6],\n",
    "        \"wv\": paramsArr[7],\n",
    "    },\n",
    "    \"gnn_layer_2\": {\n",
    "        \"b\": paramsArr[8],\n",
    "        \"we\": paramsArr[9],\n",
    "        \"wu\": paramsArr[10],\n",
    "        \"wv\": paramsArr[11],\n",
    "    },\n",
    "    \"gnn_layer_3\": {\n",
    "        \"b\": paramsArr[12],\n",
    "        \"we\": paramsArr[13],\n",
    "        \"wu\": paramsArr[14],\n",
    "        \"wv\": paramsArr[15],\n",
    "    },\n",
    "    \"linear\": {\"b\": paramsArr[16], \"w\": paramsArr[17]},\n",
    "    \"linear_1\": {\"b\": paramsArr[18], \"w\": paramsArr[19]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUROC values on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBgB_3tPbYxx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute AUROC & Create ROC curve for each scent class - uses scikit-learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "\n",
    "test_yhat = np.empty(\n",
    "    (test_N, numClasses)\n",
    ")  # create empty array to store predictions on test set\n",
    "test_y = np.empty((test_N, numClasses))\n",
    "\n",
    "\n",
    "for i, testVal in enumerate(test_set):\n",
    "    (nodes_i, edges_i), yi = testVal\n",
    "    nodes_i = nodes_i.numpy()\n",
    "    edges_i = edges_i.numpy()\n",
    "    yi = yi.numpy()\n",
    "    xi = nodes_i, edges_i\n",
    "    test_yhat[i] = jax.nn.sigmoid(model.apply(opt_params, xi))\n",
    "    test_y[i] = yi\n",
    "\n",
    "# Shape of test_yhat and test_y should be (n_samples, n_classes)\n",
    "# print(f'Shape of test_y: {np.shape(test_y)} and test_yhat: {np.shape(test_yhat)}')\n",
    "\n",
    "scentClasses_testSet = []\n",
    "aurocs_allClasses = []\n",
    "\n",
    "for c in range(numClasses):\n",
    "    if np.count_nonzero(test_y[:, c]) == 0:\n",
    "        print(f\"Test set does not have any molecules with scent {scentClasses[c]}\")\n",
    "    else:\n",
    "        ##Uncomment lines below to create ROC curves\n",
    "        # fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true = test_y[:,c], y_score = test_yhat[:,c])\n",
    "        # plt.plot(fpr, tpr, '-o', label='Trained Model')\n",
    "        # plt.plot([0,1], [0, 1], label='Naive Classifier')\n",
    "        # plt.ylabel('True Positive Rate')\n",
    "        # plt.xlabel('False Positive Rate')\n",
    "        # plt.title(f'ROC Curve for {scentClasses[c]}')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'GNN_ROC_Curve_{scentClasses[c]}.jpg')\n",
    "        # plt.close()\n",
    "        scentClasses_testSet.append(scentClasses[c])\n",
    "        auroc = sklearn.metrics.roc_auc_score(\n",
    "            y_true=test_y[:, c], y_score=test_yhat[:, c]\n",
    "        )\n",
    "        aurocs_allClasses.append(auroc)\n",
    "        print(f\"AUROC for scent {scentClasses[c]}: {auroc}\")\n",
    "\n",
    "# Write AUROC results to csv file\n",
    "# aurocTable = pd.DataFrame({'Scent': scentClasses_testSet,'AUROC': aurocs_allClasses})\n",
    "# csvFileName  = f'AurocTable_{runName}.csv'\n",
    "# aurocTable.to_csv(csvFileName,index=False)\n",
    "\n",
    "# Print Mean AUROC\n",
    "mean_AUROC = np.mean(aurocs_allClasses)\n",
    "print(f\"Mean AUROC: {mean_AUROC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that calculating AUROC for each scent class using method above & taking the mean of it is equivalent to using average=macro parameter\n",
    "auroc_sklearnAverageMacro = sklearn.metrics.roc_auc_score(\n",
    "    y_true=test_y, y_score=test_yhat, average=\"macro\"\n",
    ")\n",
    "print(mean_AUROC == auroc_sklearnAverageMacro)\n",
    "print(f\"macro-average AUROC: {auroc_sklearnAverageMacro}\")\n",
    "\n",
    "# Calculate micro-average AUROC\n",
    "auroc_sklearnAverageMicro = sklearn.metrics.roc_auc_score(\n",
    "    y_true=test_y, y_score=test_yhat, average=\"micro\"\n",
    ")\n",
    "print(f\"micro-average AUROC: {auroc_sklearnAverageMicro}\")\n",
    "\n",
    "# Calculate weighted AUROC (each class weighted by how many times it occurs in the true data sample)\n",
    "auroc_sklearnAverageWeighted = sklearn.metrics.roc_auc_score(\n",
    "    y_true=test_y, y_score=test_yhat, average=\"weighted\"\n",
    ")\n",
    "print(f\"weighted-average AUROC: {auroc_sklearnAverageWeighted}\")\n",
    "\n",
    "# Calculate median AUROC (find median value of AUROC for each scent class)\n",
    "auroc_median = np.median(aurocs_allClasses)\n",
    "print(f\"median AUROC: {auroc_median}\")\n",
    "\n",
    "# Print out values for each AUROC score value as pandas table\n",
    "tableOfAUROCValues = pd.DataFrame(index=None)\n",
    "tableOfAUROCValues[\"macro-average AUROC\"] = [auroc_sklearnAverageMacro]\n",
    "tableOfAUROCValues[\"micro-average AUROC\"] = [auroc_sklearnAverageMicro]\n",
    "tableOfAUROCValues[\"weighted-average AUROC\"] = [auroc_sklearnAverageWeighted]\n",
    "tableOfAUROCValues[\"median AUROC\"] = [auroc_median]\n",
    "tableOfAUROCValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GNN Model 3_With Haiku.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

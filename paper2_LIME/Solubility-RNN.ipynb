{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME paper: Recurrent Neural Network for Solubility Prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and set up RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "from matplotlib.offsetbox import AnnotationBbox\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import skunk\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import selfies as sf\n",
    "import exmol\n",
    "from dataclasses import dataclass\n",
    "from rdkit.Chem.Draw import rdDepictor, MolsToGridImage\n",
    "from rdkit.Chem import MolFromSmiles, MACCSkeys\n",
    "\n",
    "rdDepictor.SetPreferCoordGen(True)\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\n",
    "    \"dark\",\n",
    "    {\n",
    "        \"xtick.bottom\": True,\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.color\": \"#666666\",\n",
    "        \"ytick.color\": \"#666666\",\n",
    "        \"axes.edgecolor\": \"#666666\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"figure.dpi\": 300,\n",
    "    },\n",
    ")\n",
    "color_cycle = [\"#F06060\", \"#1BBC9B\", \"#F06060\", \"#5C4B51\", \"#F3B562\", \"#6e5687\"]\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=color_cycle)\n",
    "mpl.rcParams[\"font.size\"] = 10\n",
    "soldata = pd.read_csv(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv\"\n",
    ")\n",
    "features_start_at = list(soldata.columns).index(\"MolWt\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scramble them\n",
    "soldata = soldata.sample(frac=0.01, random_state=0).reset_index(drop=True)\n",
    "soldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfies_list = []\n",
    "for s in soldata.SMILES:\n",
    "    try:\n",
    "        selfies_list.append(sf.encoder(exmol.sanitize_smiles(s)[1]))\n",
    "    except sf.EncoderError:\n",
    "        selfies_list.append(None)\n",
    "len(selfies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic = set(exmol.get_basic_alphabet())\n",
    "data_vocab = set(\n",
    "    sf.get_alphabet_from_selfies([s for s in selfies_list if s is not None])\n",
    ")\n",
    "vocab = ['[nop]']\n",
    "vocab.extend(list(data_vocab.union(basic)))\n",
    "vocab_stoi = {o: i for o, i in zip(vocab, range(len(vocab)))}\n",
    "\n",
    "\n",
    "def selfies2ints(s):\n",
    "    result = []\n",
    "    for token in sf.split_selfies(s):\n",
    "        if token == '.':\n",
    "            continue  # ?\n",
    "        if token in vocab_stoi:\n",
    "            result.append(vocab_stoi[token])\n",
    "        else:\n",
    "            result.append(np.nan)\n",
    "            # print('Warning')\n",
    "    return result\n",
    "\n",
    "\n",
    "def ints2selfies(v):\n",
    "    return \"\".join([vocab[i] for i in v])\n",
    "\n",
    "\n",
    "# test them out\n",
    "s = selfies_list[0]\n",
    "print('selfies:', s)\n",
    "v = selfies2ints(s)\n",
    "print('selfies2ints:', v)\n",
    "so = ints2selfies(v)\n",
    "print('ints2selfes:', so)\n",
    "assert so == s.replace(\n",
    "    '.', ''\n",
    ")  # make sure '.' is removed from Selfies string during assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int\n",
    "    example_number: int\n",
    "    batch_size: int\n",
    "    buffer_size: int\n",
    "    embedding_dim: int\n",
    "    rnn_units: int\n",
    "    hidden_dim: int\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    vocab_size=len(vocab),\n",
    "    example_number=len(selfies_list),\n",
    "    batch_size=16,\n",
    "    buffer_size=10000,\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=128,\n",
    "    rnn_units=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now get sequences\n",
    "encoded = [selfies2ints(s) for s in selfies_list if s is not None]\n",
    "padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(encoded, padding=\"post\")\n",
    "\n",
    "# Now build dataset\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (padded_seqs, soldata.Solubility.iloc[[bool(s) for s in selfies_list]].values)\n",
    ")\n",
    "# now split into val, test, train and batch\n",
    "N = len(data)\n",
    "split = int(0.1 * N)\n",
    "test_data = data.take(split).batch(config.batch_size)\n",
    "nontest = data.skip(split)\n",
    "val_data, train_data = nontest.take(split).batch(config.batch_size), nontest.skip(\n",
    "    split\n",
    ").shuffle(config.buffer_size).batch(config.batch_size).prefetch(\n",
    "    tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# make embedding and indicate that 0 should be treated as padding mask\n",
    "model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=config.vocab_size, output_dim=config.embedding_dim, mask_zero=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# RNN layer\n",
    "model.add(tf.keras.layers.GRU(config.rnn_units))\n",
    "# a dense hidden layer\n",
    "model.add(tf.keras.layers.Dense(config.hidden_dim, activation=\"relu\"))\n",
    "# regression, so no activation\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(tf.optimizers.Adam(1e-4), loss=\"mean_squared_error\")\n",
    "# verbose=0 silences output, to get progress bar set verbose=1\n",
    "result = model.fit(train_data, validation_data=val_data, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"solubility-rnn-accurate\")\n",
    "# model = tf.keras.models.load_model('solubility-rnn-accurate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(result.history[\"loss\"], label=\"training\")\n",
    "plt.plot(result.history[\"val_loss\"], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"rnn-loss.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = []\n",
    "test_y = []\n",
    "for x, y in test_data:\n",
    "    yhat.extend(model(x).numpy().flatten())\n",
    "    test_y.extend(y.numpy().flatten())\n",
    "yhat = np.array(yhat)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "# plot test data\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(test_y, test_y, \":\")\n",
    "plt.plot(test_y, yhat, \".\")\n",
    "plt.text(\n",
    "    max(test_y) - 6,\n",
    "    min(test_y) + 1,\n",
    "    f\"correlation = {np.corrcoef(test_y, yhat)[0,1]:.3f}\",\n",
    ")\n",
    "plt.text(\n",
    "    max(test_y) - 6, min(test_y), f\"loss = {np.sqrt(np.mean((test_y - yhat)**2)):.3f}\"\n",
    ")\n",
    "plt.xlabel(r\"$y$\")\n",
    "plt.ylabel(r\"$\\hat{y}$\")\n",
    "plt.title(\"Testing Data\")\n",
    "plt.savefig(\"rnn-fit.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME explanations\n",
    "\n",
    "In the following example, we find out what descriptors influence solubility of a molecules. For example, let's say we have a molecule with LogS=1.5. We create a perturbed chemical space around that molecule using `stoned` method and then use `lime` to find out which descriptors affect solubility predictions for that molecule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function for RNN, to use in STONED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor function is used as input to sample_space function\n",
    "def predictor_function(smile_list, selfies):\n",
    "    encoded = [selfies2ints(s) for s in selfies]\n",
    "    # check for nans\n",
    "    valid = [1.0 if sum(e) > 0 else np.nan for e in encoded]\n",
    "    encoded = [np.nan_to_num(e, nan=0) for e in encoded]\n",
    "    padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(encoded, padding=\"post\")\n",
    "    labels = np.reshape(model.predict(padded_seqs), (-1))\n",
    "    return labels * valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptor explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure SMILES doesn't contain multiple fragments\n",
    "smi = soldata.SMILES[0]\n",
    "stoned_kwargs = {\n",
    "    \"num_samples\": 2500,\n",
    "    \"alphabet\": exmol.get_basic_alphabet(),\n",
    "    \"max_mutations\": 2,\n",
    "}\n",
    "space = exmol.sample_space(\n",
    "    smi, predictor_function, stoned_kwargs=stoned_kwargs, quiet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, SVG\n",
    "\n",
    "desc_type = [\"Classic\", \"ECFP\", \"MACCS\"]\n",
    "\n",
    "for d in desc_type:\n",
    "    beta = exmol.lime_explain(space, descriptor_type=d)\n",
    "    if d == \"Classic\":\n",
    "        exmol.plot_descriptors(space, d, output_file=f\"{d}.svg\")\n",
    "    else:\n",
    "        svg = exmol.plot_descriptors(space, d, output_file=f\"{d}.svg\")\n",
    "        plt.close()\n",
    "        skunk.display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = exmol.lime_explain(space, \"ECFP\")\n",
    "weights = exmol.plot_utils.similarity_map_using_tstats(space[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fkw = {\"figsize\": (6, 4)}\n",
    "font = {\"family\": \"normal\", \"weight\": \"normal\", \"size\": 16}\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "mpl.rc(\"axes\", titlesize=12)\n",
    "mpl.rc(\"font\", size=16)\n",
    "ax_dict = fig.subplot_mosaic(\"AABBB\")\n",
    "\n",
    "# Plot space by fit\n",
    "svg = exmol.plot_utils.plot_space_by_fit(\n",
    "    space,\n",
    "    [space[0]],\n",
    "    figure_kwargs=fkw,\n",
    "    mol_size=(200, 200),\n",
    "    offset=1,\n",
    "    ax=ax_dict[\"B\"],\n",
    "    beta=beta,\n",
    ")\n",
    "# Compute y_wls\n",
    "w = np.array([1 / (1 + (1 / (e.similarity + 0.000001) - 1) ** 5) for e in space])\n",
    "non_zero = w > 10 ** (-6)\n",
    "w = w[non_zero]\n",
    "N = w.shape[0]\n",
    "\n",
    "ys = np.array([e.yhat for e in space])[non_zero].reshape(N).astype(float)\n",
    "x_mat = np.array([list(e.descriptors.descriptors) for e in space])[non_zero].reshape(\n",
    "    N, -1\n",
    ")\n",
    "y_wls = x_mat @ beta\n",
    "y_wls += np.mean(ys)\n",
    "\n",
    "lower = np.min(ys)\n",
    "higher = np.max(ys)\n",
    "\n",
    "# set transparency using w\n",
    "norm = plt.Normalize(min(w), max(w))\n",
    "cmap = plt.cm.Oranges(w)\n",
    "cmap[:, -1] = w\n",
    "\n",
    "\n",
    "def weighted_mean(x, w):\n",
    "    return np.sum(x * w) / np.sum(w)\n",
    "\n",
    "\n",
    "def weighted_cov(x, y, w):\n",
    "    return np.sum(w * (x - weighted_mean(x, w)) * (y - weighted_mean(y, w))) / np.sum(w)\n",
    "\n",
    "\n",
    "def weighted_correlation(x, y, w):\n",
    "    return weighted_cov(x, y, w) / np.sqrt(\n",
    "        weighted_cov(x, x, w) * weighted_cov(y, y, w)\n",
    "    )\n",
    "\n",
    "\n",
    "corr = weighted_correlation(ys, y_wls, w)\n",
    "\n",
    "ax_dict[\"A\"].plot(\n",
    "    np.linspace(lower, higher, 100), np.linspace(lower, higher, 100), \"--\", linewidth=2\n",
    ")\n",
    "sc = ax_dict[\"A\"].scatter(ys, y_wls, s=50, marker=\".\", c=cmap, cmap=cmap)\n",
    "ax_dict[\"A\"].text(max(ys) - 3, min(ys) + 1, f\"weighted \\ncorrelation = {corr:.3f}\")\n",
    "ax_dict[\"A\"].set_xlabel(r\"$\\hat{y}$\")\n",
    "ax_dict[\"A\"].set_ylabel(r\"$g$\")\n",
    "ax_dict[\"A\"].set_title(\"Weighted Least Squares Fit\")\n",
    "ax_dict[\"A\"].set_xlim(lower, higher)\n",
    "ax_dict[\"A\"].set_ylim(lower, higher)\n",
    "ax_dict[\"A\"].set_aspect(1.0 / ax_dict[\"A\"].get_data_ratio(), adjustable=\"box\")\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.Oranges, norm=norm)\n",
    "cbar = plt.colorbar(sm, orientation=\"horizontal\", pad=0.15, ax=ax_dict[\"A\"])\n",
    "cbar.set_label(\"Chemical similarity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"weighted_fit.svg\", dpi=300, bbox_inches=\"tight\", transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness to incomplete sampling\n",
    "\n",
    "We first sample a reference chemical space, and then subsample smaller chemical spaces from this reference. Rank correlation is computed between important descriptors for the smaller subspaces and the reference space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a big space\n",
    "stoned_kwargs = {\n",
    "    \"num_samples\": 5000,\n",
    "    \"alphabet\": exmol.get_basic_alphabet(),\n",
    "    \"max_mutations\": 2,\n",
    "}\n",
    "space = exmol.sample_space(\n",
    "    smi, predictor_function, stoned_kwargs=stoned_kwargs, quiet=True\n",
    ")\n",
    "len(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptor attributions\n",
    "exmol.lime_explain(space, \"MACCS\", return_beta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign feature ids for rank comparison\n",
    "features = features = {\n",
    "    a: b\n",
    "    for a, b in zip(\n",
    "        space[0].descriptors.descriptor_names,\n",
    "        np.arange(len(space[0].descriptors.descriptors)),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set of ranks for the reference space\n",
    "baseline_imp = {\n",
    "    a: b\n",
    "    for a, b in zip(space[0].descriptors.descriptor_names, space[0].descriptors.tstats)\n",
    "    if not np.isnan(b)\n",
    "}\n",
    "baseline_imp = dict(\n",
    "    sorted(baseline_imp.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    ")\n",
    "baseline_set = [features[x] for x in baseline_imp.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get subsets and calculate lime importances - subsample - get rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "N = len(space)\n",
    "size = np.arange(500, N, 1000)\n",
    "rank_corr = {N: 1}\n",
    "for i, f in enumerate(size):\n",
    "    # subsample space\n",
    "    rank_corr[f] = []\n",
    "    for _ in range(10):\n",
    "        # subsample space of size f\n",
    "        idx = np.random.choice(np.arange(N), size=f, replace=False)\n",
    "        subspace = [space[i] for i in idx]\n",
    "        # get desc attributions\n",
    "        ss_beta = exmol.lime_explain(subspace, descriptor_type=\"MACCS\")\n",
    "        ss_imp = {\n",
    "            a: b\n",
    "            for a, b in zip(\n",
    "                subspace[0].descriptors.descriptor_names, subspace[0].descriptors.tstats\n",
    "            )\n",
    "            if not np.isnan(b)\n",
    "        }\n",
    "        ss_imp = dict(\n",
    "            sorted(ss_imp.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "        )\n",
    "        ss_set = [features[x] for x in ss_imp.keys()]\n",
    "        # Get ranks for subsampled space and compare with reference\n",
    "        ranks = {a: [b] for a, b in zip(baseline_set[:5], np.arange(1, 6))}\n",
    "        for j, s in enumerate(ss_set):\n",
    "            if s in ranks:\n",
    "                ranks[s].append(j + 1)\n",
    "        # compute rank correlation\n",
    "        r = spearmanr(np.arange(1, 6), [ranks[x][1] for x in ranks])\n",
    "        rank_corr[f].append(r.correlation)\n",
    "\n",
    "    plt.scatter(f, np.mean(rank_corr[f]), color=\"#13254a\", marker=\"o\")\n",
    "\n",
    "plt.scatter(N, 1.0, color=\"red\", marker=\"o\")\n",
    "plt.axvline(x=N, linestyle=\":\", color=\"red\")\n",
    "plt.xlabel(\"Size of chemical space\")\n",
    "plt.ylabel(\"Rank correlation\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rank correlation.svg\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of mutation number, alphabet and size of chemical space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mutation\n",
    "desc_type = [\"Classic\"]\n",
    "muts = [1, 2, 3]\n",
    "for i in muts:\n",
    "    stoned_kwargs = {\n",
    "        \"num_samples\": 2500,\n",
    "        \"alphabet\": exmol.get_basic_alphabet(),\n",
    "        \"min_mutations\": i,\n",
    "        \"max_mutations\": i,\n",
    "    }\n",
    "    space = exmol.sample_space(\n",
    "        smi, predictor_function, stoned_kwargs=stoned_kwargs, quiet=True\n",
    "    )\n",
    "    for d in desc_type:\n",
    "        exmol.lime_explain(space, descriptor_type=d)\n",
    "        exmol.plot_descriptors(\n",
    "            space, d, output_file=f\"desc_{d}_mut{i}.svg\", title=f\"Mutations={i}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Alphabet\n",
    "basic = exmol.get_basic_alphabet()\n",
    "train = sf.get_alphabet_from_selfies([s for s in selfies_list if s is not None])\n",
    "wide = sf.get_semantic_robust_alphabet()\n",
    "desc_type = [\"MACCS\"]\n",
    "alphs = {\"Basic\": basic, \"Training Data\": train, \"SELFIES\": wide}\n",
    "for a in alphs:\n",
    "    stoned_kwargs = {\"num_samples\": 2500, \"alphabet\": alphs[a], \"max_mutations\": 2}\n",
    "    space = exmol.sample_space(\n",
    "        smi, predictor_function, stoned_kwargs=stoned_kwargs, quiet=True\n",
    "    )\n",
    "    for d in desc_type:\n",
    "        exmol.lime_explain(space, descriptor_type=d)\n",
    "        svg = exmol.plot_descriptors(\n",
    "            space, d, output_file=f\"desc_{d}_alph_{a}.svg\", title=f\"Alphabet: {a}\"\n",
    "        )\n",
    "        plt.close()\n",
    "        skunk.display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of space\n",
    "desc_type = [\"MACCS\"]\n",
    "space_size = [1500, 2000, 2500]\n",
    "for s in space_size:\n",
    "    stoned_kwargs = {\n",
    "        \"num_samples\": s,\n",
    "        \"alphabet\": exmol.get_basic_alphabet(),\n",
    "        \"max_mutations\": 2,\n",
    "    }\n",
    "    space = exmol.sample_space(\n",
    "        smi, predictor_function, stoned_kwargs=stoned_kwargs, quiet=True\n",
    "    )\n",
    "    for d in desc_type:\n",
    "        exmol.lime_explain(space, descriptor_type=d)\n",
    "        svg = exmol.plot_descriptors(\n",
    "            space,\n",
    "            d,\n",
    "            output_file=f\"desc_{d}_size_{a}.svg\",\n",
    "            title=f\"Chemical space size={s}\",\n",
    "        )\n",
    "        plt.close()\n",
    "        skunk.display(svg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abc1ef2aae668f29add333aedc207234808b19831866b8480f007a054a2482dc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
